\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{color}
\usepackage{listings}

\graphicspath{/home/alexis/git/Spam-Filter-project/doc/}

\author{Prisca Aeby, Alexis Semple}
\title{Naive Bayes Spam Filter}

\pagestyle{fancy}
\fancyhf{}
\rhead{Project report}
\lhead{P. Aeby, A. Semple}
\chead{Naive Bayes Spam Filter}
\cfoot{\thepage}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

% Paragraph format
\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

% Hyperlink format
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\linespread{1.1}

\renewcommand\thesection{\arabic{section}}
\renewcommand{\arraystretch}{1.5}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE University of Bristol}\\[1.5cm] % Name of your university/college
\textsc{\LARGE Introduction to Machine Learning}\\[1cm] % Major heading such as course name
\textsc{\Large Project report}\\[1cm]

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{\huge \bfseries Naive Bayes Spam Filter}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\Large \emph{Authors:}\\
Prisca Aeby, Alexis Semple\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\begin{abstract}
With today's world becoming more and more flooded with e-mails, spammers are responding with an alarming increase of unsolicited messages. The natural reaction to this has been the development of many different spam filtering techniques, of varying efficiency and reliability. We are going to be looking more specifically at one of these techniques, Naive Bayes spam filtering, how it can be implemented and improved, and how it compares to other filtering techniques.
\end{abstract}

\tableofcontents \pagebreak

\section{Introduction}
Naive Bayes text classification is a popular technique used in spam e-mail filtering. It relies on correlating the use of tokens (in our case words) and then using Bayesian inference to calculate the probability that a given e-mail is spam or ham. \footnote{\href{http://www.wikiwand.com/en/Naive\_Bayes\_spam\_filtering}{Wikipedia on Naive Bayes spam filtering} -  http://www.wikiwand.com/en/Naive\_Bayes \_spam\_filtering}
Naive Bayes classifiers suppose strong independence between the features of the model.

In the case of our project, the goal was to develop a binomial Naive Bayes text classifier written in Java to be tested and trained on real world data, or more specifically a large set of real world e-mails.

\section{The Algorithm}
We based the main algorithm of our spam filter on  \texttt{\textsc{Learn\_naive\_Bayes\_text}} and \texttt{\textsc{Classify\_naive\_Bayes\_text}} found in Tom Mitchell's book (on page 183).\footnote{\href{http://personal.disco.unimib.it/Vanneschi/McGrawHill\_-\_Machine\_Learning\_-Tom\_Mitchell.pdf}{http://personal.disco.unimib.it/Vanneschi/McGrawHill\_-\_Machine\_Learning\_-Tom\_Mitchell.pdf}} 
Our program is split into two parts.

\subsection{Training}
The file \texttt{filter\_train.java} runs all of the computations and produces the results necessary to the supervised training part of the Naive Bayes text classification algorithm. In it, we iterate over all the files contained in a 'training set' directory and create a \texttt{java.util.HashMap} of all of the words therein. Each word is then a key in the HashMap and is associated with its value, a static array, each containing 4 precise values in the following order (\texttt{double[0..3]}):
\begin{itemize}
\item \texttt{double[0]}: The number of times the word has occurred in all files marked 'spam*' in the training data.
\item \texttt{double[1]}: The number of times the word has occurred in all files marked 'ham*' in the training data.
\item \texttt{double[2]}: The probability that a randomly drawn word from a document in class $Spam$ will be this word $w_k \rightarrow P(w_k|Spam)$
\item \texttt{double[3]}: The probability that a randomly drawn word from a document in class $Ham$ will be this word $w_k \rightarrow P(w_k|Ham)$
\end{itemize} 
where we have $pSpam$ and $pHam$ respectively being computed by dividing the number of files marked spam* (or ham*) over the total number of training files. The values of \texttt{double[2]} and \texttt{double[3]} as explained above are computed as follows: 
\begin{lstlisting}
for(String word: vocabulary.keySet()){
	double[] tab = vocabulary.get(word);
	tab[2] = (tab[0] + 1) / ((double)(totalWords[0] + vocabulary.size()));
	tab[3] = (tab[1] + 1) / ((double)(totalWords[1] + vocabulary.size()));
	vocabulary.put(word, tab);
		}
\end{lstlisting}
where \texttt{totalWords[0..1]} contains the total number of words in all spam (resp. all ham) files and \texttt{vocabulary} is the HashMap of all words. According to the project description, we made the word selection process case-sensitive, and also decided to interpret any form of punctuation or whitespace as a word separator. So the scanner iterating over all words in file \texttt{f} behaves like such:
\begin{lstlisting}
scanner = new Scanner(f).useDelimiter("[\\s\\p{Punct}]+");
\end{lstlisting}

The choice concerning punctuation was a personal one and based on the assumption that we had enough training data provided to ignore varying forms of punctuation. Results may have varied slightly had we taken these different forms into account, particularly considering use that is excessive and non-conform to standards which is dominant in some types of spam.

At the end of the program, the content of vocabulary is written to a text file named \texttt{trainging\_data.txt} in a specific format: the first line contains $pSpam$ and $pHam$ in that order, separated by a space. Each of the following lines is dedicated to a single entry of the \texttt{vocabulary} HashMap, starting by the word itself, followed by the four values of its corresponding array, each separated by a space. The result looks something like this: 
\begin{lstlisting}
...
invested 7.0 0.0 2.088031403992316E-5 4.980206170575049E-7 
...
\end{lstlisting}
meaning that for the word 'invested' we have that it appears 7 times in files marked 'spam*', 0 times in files marked 'ham*', has a probability of $2.088031403992316 * 10^{-5}$ of being drawn at random from class $Spam$ and a probability of $4.980206170575049 * 10^{-7}$ of being drawn at random from class $Ham$. One obvious observation is that these probabilities are extremely small and can pose problems of arithmetic underflow later on, in the classification part of the algorithm. The solution we implemented to avoid this is to use logarithms, which converts very small positive numbers into large negatives. So the code given above for computing \texttt{double[2]} and \texttt{double[3]} becomes 

\begin{lstlisting}
...
tab[2] = Math.log(tab[0] + 1) / ((double)(totalWords[0] + vocabulary.size())));
tab[3] = Math.log((tab[1] + 1) / ((double)(totalWords[1] + vocabulary.size())));
...
\end{lstlisting}

and the much more reasonable result in the text file is

\begin{lstlisting}
...
invested 7.0 0.0 -10.776703754836734 -14.512624361064837 
...
\end{lstlisting}

\subsection{Classification}
Once the training is completed, the program in \texttt{filter.java} can then use the results in \texttt{training\_data.txt} to rebuild a Hashmap of \texttt{<word, double[]>} and classify instances provided to it. The formula provided in Mitchell's book returns an estimated target value for a given instance that is being tested. For spam
\begin{center}
$ p_1 = pSpam\prod\limits_{w_k \in vocabulary}P(w_k|Spam)$
\end{center} 
and for ham
\begin{center}
$ p_2 = pHam\prod\limits_{w_k \in vocabulary}P(w_k|Ham)$
\end{center} 
and the return value is 
\begin{lstlisting}
if(max(p1, p2) == p1) System.out.println("spam\n");
else System.out.println("ham\n");
\end{lstlisting}
However, since in the training part we modified $P(w_k|Spam)$ to $log(P(w_k|Spam))$ (resp. $P(w_k|Ham)$ to $log(P(w_k|Ham))$) we get 
\begin{center}
$ p_1 = log(pSpam) + \sum\limits_{w_k \in vocabulary}log(P(w_k|Spam))$
\end{center}
with the same going for $p_2$, obviously. This determines whether a test instance is classified as spam (for $p_1 > p_2$) or ham (otherwise).

\section{Improving the classifier and testing}
\subsection{Simple preprocessing}
As indicated by the project specification, in order to test the consistency of the classifier and to evaluate its precision, we performed a 10-fold cross-validation on the provided training set, which means that we split the training set into 10 folds of equal size, and we did so randomly in order to obtain an approximately equal spread of members of both classes in every test sets. To each test set (10\% of the original training set, i.e. 250 files) we then match the remaining 90\% as a training set. So we end up being able to test our classifier on 10 different folds with 250 test instances and different training sets paired with them.

We have provided the bash scripts that we used to perform the random splitting into 10 test-training pairs (called \texttt{divide\_train.bash}) and the cross-validation (called \texttt{cross\_validation.bash}). The latter outputs a result file for each fold with the number of successful classifications and the total number of test instances. It also saves every misclassified instance to a separate folder for further examination.

\subsubsection{10-fold cross-validation}

To begin with, we ran a cross-validation on our Naive Bayesian classifier as described up to this point, that is to say with no text preprocessing steps and using whitespaces and any punctuation mark as word separators. Following this, we ran another cross-validation, this time considering all punctuation marks as part of the word rather than as a separator. The results can be seen in figure 1.

\begin{figure}
\includegraphics[width=\textwidth]{1st_preprocessing}
\centering
\caption{Results of the first two 10-fold cross-validations. On the left, we have the Naive Bayes classifier using white spaces and punctuation as separators. On the right, Naive Bayes using only white spaces.}
\end{figure}

\paragraph*{Paired t-test}
We can see that the second classifier seems to be slightly more efficient, with a slightly higher mean success rate ($97.04\%$) and a lower standard deviation ($\approx0.015$). To be able to conclusively decide whether this observed higher performance is due to chance or superiority of the classifier, we'll compute the \textit{p-value} for this pair of results. We first state our null and alternative hypotheses:
\begin{center}
$H_0: $ true difference in means is equal to 0

$H_1: $ true difference in means is not equal to 0
\end{center}
In other words, if our test accepts the null hypothesis, it indicates that the difference between the results of the two 10-fold cross-validations are due to random behaviour of the data. As is customary, we'll choose 95\% confidence interval, so the test will decide whether or not the differences are significant at the $\alpha = 0.05$ level.

The \textit{p-value} can be computed easily using R. The result for the following commands is then given as follows (NB is Naive Bayes, NBwp is Naive Bayes with punctuation as word part):

\begin{lstlisting}[language=R]
> NB <- c(0.952, 0.976, 0.968, 0.948, 0.972, 0.980, 0.976, 0.976, 0.94, 0.932)
> NBwp <- c(0.956, 0.988, 0.964, 0.964, 0.98, 0.98, 0.972, 0.988, 0.972, 0.94)
> t.test(NB, NBwp, paired=TRUE)

Paired t-test

data:  NB and NBwp
t = -2.473, df = 9, p-value = 0.0354
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.0160839381 -0.0007160619
sample estimates:
mean of the differences 
                -0.0084

\end{lstlisting}

The R program gives us several results, e.g. the t-statistic (= -2.473), the degrees of freedom (= 9), the confidence interval with $\alpha = 0.05$, but most importantly, we can tell from the fact that the p-value is $0.0354 < 0.05$ that we can reject $H_0$ and say with a confidence of 95\% that the difference between these classifiers is not due to chance. Considering punctuation marks as part of a word yields a better spam-filter, and this is the technique we adopted for our filter.

A reason for the superiority of this technique in this context could be the use of certain words paired with strong punctuation could be very decisive indicators for one class or the other (some simple examples could be 'viagra!', 'investment?', 'lottery!!!', etc.).

\paragraph*{Friedman test}
In order to further test these results, we ran a Friedman test on the data, and obtained the following 

\begin{lstlisting}[language=R]
> NB <- c(0.952, 0.976, 0.968, 0.948, 0.972, 0.98, 0.976, 0.976, 0.94, 0.932)
> NBp <- c(0.956, 0.988, 0.964, 0.964, 0.98, 0.98, 0.972, 0.988, 0.972, 0.94)
> F1 = matrix(
+ c(NB, NBp),
+ nrow=10,
+ ncol=2)

> friedman.test(F1)

	Friedman rank sum test

data:  F1
Friedman chi-squared = 2.7778, df = 1, p-value = 0.09558
\end{lstlisting}

The Friedman test ranks each row of a matrix according to the highest performance. It then tests whether differences between the mean ranking for each column is significant or not (again here at level $ \alpha = 0.05 $). It has the following null hypothesis:
 
\begin{center}
$ H_0: $ All algorithms perform equally.
\end{center}

This can be rejected only if the critical value is less or equal to the Friedman statistic. In the results above, we can see that the Friedman statistic is 2.7778, and by looking it up in a chi-squared table for $ \alpha = 0.05, df = 1 $, we find a critical value of $ 3.84 \geq 2.7778 $. So we have to accept $H_0$ which states that the two methods perform essentially with the same efficiency.

This can be seen to be contradictory to our previous results, since we had found that including punctuation marks into the words was seen to yield superior classification efficiency. However, we could also remark that in both tests, the result is relatively close to the threshold (i.e. the chosen $\alpha$ and critical value), so the outcome can legitimately vary from one test to the next.

\subsection{Advanced preprocessing techniques}
To continue to try and improve our filter, we implemented and tested some other preprocessing techniques that could result in higher success rates. 

\subsubsection{Stop-words removal}
This technique consists in removing from the set of all words in the data some basic words that would result in inconclusive evidence to either the spam or ham class. In most cases, these words are common function words, but there is no defined standard set. In our case, we decided to remove the entries of our stop-words once our \texttt{vocabulary} HashMap was complete:

\begin{lstlisting}
ArrayList<String> stopWords = new ArrayList<String>(Arrays.asList("a", "an", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "such", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with"));
        for(String word: vocabulary.keySet()){
                if(stopWords.contains(word)){
                        vocabulary.remove(word);
                }
\end{lstlisting}

\subsubsection{Stemming} 
The stemming preprocessing technique consists in reducing words to their morphological root. This does not always result in the stem being the actual root of the word or it being an actual word. For example the words "argue", "argument", "arguing", "argues", "argus" would all be stemmed to "argu", which is neither a real world, nor the actual root of the listed words.
\footnote{\href{http://www.wikiwand.com/en/Stemming}{Wikipedia on Stemming} -  http://www.wikiwand.com/en/Stemming} 

We implemented this technique in our filter using the Snowball Stemmer for Java.
\footnote{\href{http://trimc-nlp.blogspot.co.uk/2013/08/snowball-stemmer-for-java.html}{The .jar can be downloaded from this website} -  http://trimc-nlp.blogspot.co.uk/2013/08/snowball-stemmer-for-java.html}
This resulted in us adding the stem of the word in our \texttt{vocabulary} collection, rather than the actual word. As a consequence, the training data would then be quite a lot smaller than without the stemming technique ($\approx 4.1$ Mb rather than $\approx 4.7$ Mb without stemming).

\subsubsection{10-fold cross-validation} 

We ran cross validations on three combinations of these techniques, using stop-words and stemming alone and then together. The results can be seen in figures 2 and 3.

\begin{figure}
\includegraphics[width=\textwidth]{advanced1st}
\centering
\caption{Results of the Naive Bayes and Naive Bayes with stop-words removal cross-validations.}
\end{figure}

\paragraph*{Paired t-test}
As for the previous cross-validations, we want to test these results at a level of significance of $\alpha = 0.05$. We ran the same R commands as previously, pairwise to compare with the Naive Bayes filter without any preprocessing and obtained the following:
\begin{itemize}
\item \textbf{NB - NB with stop-words removal}: $p-value = 0.5203$, $H_0$ is accepted.
\item \textbf{NB - NB with stemming}: $p-value = 0.0001733$, $H_0$ is rejected.
\item \textbf{NB - NB with stop-words removal and stemming}: \linebreak $p-value = 0.0003579$, $H_0$ is accepted.
\end{itemize}

From this, we know that we can assume with a confidence level of 95\% that stemming preprocessing has a negative impact on the performance of the filter, whereas stop-words removal seems to have a positive effect, but the p-value points to the difference being due to chance and therefore non-conclusive.

A way to interpret the failure of stemming to improve the filter is to look at both the smaller size of the training data and the relatively weaker performance as an instance of generalisation of the data by the classifier, i.e. it reduces the words to a too general state and they lose information relevant to their probability of occurring in the respective classes of the model.

\begin{figure}
\includegraphics[width=\textwidth]{advanced2nd}
\centering
\caption{Results of the Naive Bayes with stemming and Naive Bayes with stemming and stop-words removal preprocessing.}
\end{figure}

\paragraph*{Friedman test}
Once again we ran a Friedman test on these results to see if it validates our findings.
\begin{itemize}
\item \textbf{Test on all four cross-validations:} Friedman statistic = 24.7419, for $ \alpha = 0.05, df = 3 $, critical value = 7.81, $ H_0 $ is rejected.
\item \textbf{Test on NB and NB with stop-words removal:} Friedman statistic = 0.5, for $ \alpha = 0.05, df = 1 $, critical value = 3.84, $ H_0 $ is accepted.
\item \textbf{Test on NB and NB with stemming:} Friedman statistic = 9, for $ \alpha = 0.05, df = 1 $, critical value = 3.84, $ H_0 $ is rejected.
\end{itemize}

Briefly summarised, these results concur with our t-test results, since they state that all 4 methods compared together do not perform equally, nor do Naive Bayes without advanced preprocessing and Naive Bayes with stemming. However, when comparing the basic Naive Bayes with the one with stop-words removal, the null hypothesis is accepted, and the test states that both rank equally, with a level of confidence of 95\%.

\section{Other classifiers}
In order to evaluate the efficiency of our classifier within the range of other widely used classifiers in the real world, we ran two more cross-validations, using two of WEKA's defined classifiers: the SMO implementation of support vector machine classifying, and the J48 implementation of Decision Tree classification. The results can be seen in figure 4.

A support vector machine model constructs a hyperplane in a n-dimensional space (in our case n = 2) mapped so that the n categories are divided by as wide a gap as possible, hence resulting in a low generalisation error for the classifier.\footnote{\href{http://www.wikiwand.com/en/Support\_vector\_machine}{Wikipedia on Support vector machines - http://www.wikiwand.com/en/Support\_vector\_machine}} 

A decision tree model is constructed by splitting a source set (i.e. the training set) into subsets based on an attribute value test.\footnote{\href{http://www.wikiwand.com/en/Decision\_tree\_learning}{Wikipedia on Decision tree learning - http://www.wikiwand.com/en/Decision\_tree\_learning}}

\begin{figure}
\includegraphics[width=\textwidth]{Other_class}
\centering
\caption{Results of the SVM and Decision Tree classifier cross-validations.}
\end{figure}

\paragraph*{Paired t-test}
As was done previously, we ran a paired t-test on the results of these two classifiers with the results of the basic Naive Bayes classifier given in figure 2. Using R, we obtained the following results:

\begin{itemize}
\item \textbf{NB - SVM:} $ p-value = 0.6808 $, $ H_0 $ is accepted.
\item \textbf{NB - J48 Decision tree:} $ p-value = 0.0001175 $, $ H_0 $ is rejected. 
\end{itemize}

The results of the test state that only for the NB - Decision tree comparison is the p-value significant at the $ \alpha = 0.05 $ level, and therefore the differences in performance between the two algorithms are not due to chance but rather to the superiority of the Naive Bayes classifier, over the Decision Tree.

\paragraph*{Friedman test}
From the above results, we would expect the Friedman test to yield a result that would reject the null hypothesis. We tested this as previously, using R.

\begin{itemize}
\item \textbf{Global test on NB, SVM, J48:} Friedman statistic = 15.2, for $ \alpha = 0.05, df = 2 $, critical value = 5.99, $ H_0 $ is rejected.
\item \textbf{Test on NB and SVM:} Friedman statistic = 0.4, for $ \alpha = 0.05, df = 1 $, critical value = 3.84, $ H_0 $ is accepted.
\end{itemize}

As expected, the results to the global test suggest that the three algorithms don't behave equally, but NB and SVM are accepted at a confidence level of 95\% to perform equally.

\section{Additional observations}
\subsection{High word occurrences in the different classes}
We wanted to investigate whether any information could be obtained from the high frequency words occurring in the spam and ham classes respectively, so we extracted the 20 highest occurrence rates for each class and its corresponding key value from our \texttt{vocabulary} HashMap, trying different settings for our filter in order to establish the variance in the results (i.e. punctuation as word separator, stop-words removal, setting a minimum length for the word).

These results yield feeble observations compared to what we have been able to establish so far, and were unable to draw any reasonable conclusions from it. We have nonetheless added the results as an annexed text file with the submission documents named 'word\_analysis.txt'. 

One of the searches, for the most occurring words in the spam and ham classes yielded the following. This result is for the classifier using the simple preprocessing of counting punctuation as part of words, and an imposed lower bound for word length. 

\begin{lstlisting}[language=]
********Punctuation as part of word, without stop words removal, with word limit (>5)********

-Spam words:

{eived:=1013.0, people=446.0, program=385.0, please=371.0, address=361.0, addresses=297.0, href="h=295.0, zzzz@lo=284.0, Delivered-To:=274.0, e-mail=265.0, Message-Id:=225.0, millio=201.0, remove=190.0, -Type:=188.0, orders=180.0, [127.0.0.1])=169.0, sfer-E=166.0, MIME-Versio=157.0, hmail-5.9.0)=148.0, (8.9.3/8.9.3)=146.0, <zzzz@lo=142.0, .org>;=136.0, blishi=133.0, .spamassassi=129.0, message=128.0, available=124.0, removed=123.0, ll.org=121.0, Reply-To:=113.0, Normal=112.0, X-Mailer:=108.0, dogma.slash=104.0, follow=103.0, followi=102.0, simply=99.0, <zzzz@spamassassi=97.0, before=96.0}

-Ham words:

{eived:=7582.0, href="h=6859.0, om/b.gif"=5911.0, p://www.=5571.0, om/Cli=4639.0, p://www.zd=3343.0, ellspa=3315.0, ellpaddi=3304.0, ha.org=2502.0, border="0"=2267.0, g-admi=2233.0, p://home.=2134.0, [127.0.0.1])=1925.0, e="arial,=1713.0, bsp;<a=1702.0, ="1"></=1638.0, (8.11.6/8.11.6)=1540.0, iversi=1437.0, -Type:=1327.0, (8.9.3/8.9.3)=1325.0, ll.org=1299.0, dogma.slash=1292.0, Delivered-To:=1266.0, .org>;=1238.0, /////////////////////////////////////////////////////// /////////////////////=1214.0, p://www.lo=1154.0, Users'=1128.0, [127.0.0.1]=1107.0, Message-Id:=1056.0, hmail-5.9.0)=994.0, MIME-Versio=978.0, olor="#000000"><img=974.0, h="10"=952.0, x.ie>;=945.0, HREF="h=935.0, border=0=929.0, ="_bla=928.0, border="0">=910.0, om/graphi=847.0}


\end{lstlisting}

\subsection{Significant error types}
Another additional type of performance analysis we looked at, was evaluating any observable differences in certain errors among the classifiers, i.e. false negatives and false positives. The higher cost error of the two is quite clearly the false positive, since classifying the occasional spam e-mail as ham is something easily spotted by the user, whereas a ham e-mail set in the spam class will not necessarily be found, since the tendency to verify all spam is much smaller on the user end. The error of a false positive is therefore much more likely to unnoticed. As a result, we focused mainly on the latter type of error.

\subsubsection{False positives}

The number of false positives, their means and standard deviations for different preprocessing combinations of our Naive Bayes classifier can be found in figure 5. These statistics visibly confirm what we had already obtained in our previous tests.

First of all, the stemming preprocessing technique does not yield encouraging results and in some instances of its use, it yields alarmingly high rates of false positives, going up to the maximum of 23 in our cross validations. 

\begin{figure}
\includegraphics[width=\textwidth]{false_positives}
\centering
\caption{Count of false positive errors per classifier. Notation: NB = Naive Bayes, sw = stop-words removal, st = stemming, p = Punctuation as word part}
\end{figure}

Again we can remark that including punctuation in the words of our vocabulary rather than seeing it as a word separator globally lowers the rate of false positives, and the best results are achieved with only that simple preprocessing technique (on average 3.3 false positives per fold). 

The use of stop-words removal seems to slightly change the level of false positive occurrence, in the basic case, using punctuation as word separators, it improves the result, in the other, it worsens them. The efficiency of this technique seems to vary according to which other techniques are applied.

\section{Conclusion}
The creation of this spam filter has lead us to appreciate the efficiency of a simple Naive Bayes text classifier which is trained with a large enough data set. It competes surprisingly well with other classifiers that one would have a tendency to call more sophisticated. From our statistical analysis, we found that our implementation of a multinomial Naive Bayes filter can be superior to widely used classifiers such as decision trees, and was found to perform on an equal level with a support vector machine.

Improving a classifier is something that requires a lot of testing as well, and we've established that some preprocessing techniques perform better in the company or absence of others. This needs to be studied carefully before integrating them into a classifier.

There are of course a lot of other preprocessing techniques which we didn't try to implement or study the efficiency of, but some of them would probably increase our overall performance, for example, systematically removing header information in e-mails, in order to avoid heavy repetition of some words yielding very little information as to the class of the instance (much like stop-words). This suggests that an even higher accuracy rate can be achieved by using quite simple techniques on an overall simple classifier.

\end{document}